{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import logging\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "import config\n",
    "from ingredient_parser import ingredient_parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "import string\n",
    "import ast\n",
    "import re\n",
    "import unidecode\n",
    "import nltk.corpus as corpus\n",
    "\n",
    "#nltk.download('omw-1.4')\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "from collections import Counter\n",
    "#import config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append('..')\n",
    "\n",
    "if sys.platform == 'linux':\n",
    "    path = config.LINUX_PATH\n",
    "else:\n",
    "    path = config.OS_PATH\n",
    "os.chdir(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append('..')\n",
    "path = config.OS_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>steps</th>\n",
       "      <th>ingredients</th>\n",
       "      <th>ingredients_parsed</th>\n",
       "      <th>parsed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>arriba   baked winter squash mexican style</td>\n",
       "      <td>['make a choice and proceed with recipe', 'dep...</td>\n",
       "      <td>['winter squash', 'mexican seasoning', 'mixed ...</td>\n",
       "      <td>winter squash mexican seasoning mixed spice ho...</td>\n",
       "      <td>[winter squash, mexican seasoning, mixed spice...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>a bit different  breakfast pizza</td>\n",
       "      <td>['preheat oven to 425 degrees f', 'press dough...</td>\n",
       "      <td>['prepared pizza crust', 'sausage patty', 'egg...</td>\n",
       "      <td>prepared pizza crust sausage patty egg milk sa...</td>\n",
       "      <td>[prepared pizza crust, sausage patty, eggs, mi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>beat this  banana bread</td>\n",
       "      <td>['preheat oven to 350 degrees', 'butter two 9x...</td>\n",
       "      <td>['sugar', 'unsalted butter', 'bananas', 'eggs'...</td>\n",
       "      <td>sugar unsalted butter banana egg fresh lemon j...</td>\n",
       "      <td>[sugar, unsalted butter, bananas, eggs, fresh ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>better than sex  strawberries</td>\n",
       "      <td>['crush vanilla wafers into fine crumbs and li...</td>\n",
       "      <td>['vanilla wafers', 'butter', 'powdered sugar',...</td>\n",
       "      <td>vanilla wafer butter powdered sugar egg whippi...</td>\n",
       "      <td>[vanilla wafers, butter, powdered sugar, eggs,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>better then bush s  baked beans</td>\n",
       "      <td>['in a very large sauce pan cover the beans an...</td>\n",
       "      <td>['great northern bean', 'chicken bouillon cube...</td>\n",
       "      <td>great northern bean chicken bouillon cube dark...</td>\n",
       "      <td>[great northern bean, chicken bouillon cubes, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         name  \\\n",
       "0  arriba   baked winter squash mexican style   \n",
       "1            a bit different  breakfast pizza   \n",
       "2                     beat this  banana bread   \n",
       "3               better than sex  strawberries   \n",
       "4             better then bush s  baked beans   \n",
       "\n",
       "                                               steps  \\\n",
       "0  ['make a choice and proceed with recipe', 'dep...   \n",
       "1  ['preheat oven to 425 degrees f', 'press dough...   \n",
       "2  ['preheat oven to 350 degrees', 'butter two 9x...   \n",
       "3  ['crush vanilla wafers into fine crumbs and li...   \n",
       "4  ['in a very large sauce pan cover the beans an...   \n",
       "\n",
       "                                         ingredients  \\\n",
       "0  ['winter squash', 'mexican seasoning', 'mixed ...   \n",
       "1  ['prepared pizza crust', 'sausage patty', 'egg...   \n",
       "2  ['sugar', 'unsalted butter', 'bananas', 'eggs'...   \n",
       "3  ['vanilla wafers', 'butter', 'powdered sugar',...   \n",
       "4  ['great northern bean', 'chicken bouillon cube...   \n",
       "\n",
       "                                  ingredients_parsed  \\\n",
       "0  winter squash mexican seasoning mixed spice ho...   \n",
       "1  prepared pizza crust sausage patty egg milk sa...   \n",
       "2  sugar unsalted butter banana egg fresh lemon j...   \n",
       "3  vanilla wafer butter powdered sugar egg whippi...   \n",
       "4  great northern bean chicken bouillon cube dark...   \n",
       "\n",
       "                                              parsed  \n",
       "0  [winter squash, mexican seasoning, mixed spice...  \n",
       "1  [prepared pizza crust, sausage patty, eggs, mi...  \n",
       "2  [sugar, unsalted butter, bananas, eggs, fresh ...  \n",
       "3  [vanilla wafers, butter, powdered sugar, eggs,...  \n",
       "4  [great northern bean, chicken bouillon cubes, ...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('input/df_recipes.csv')\n",
    "data['parsed'] = data.ingredients.apply(ingredient_parser)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['winter squash',\n",
       " 'mexican seasoning',\n",
       " 'mixed spice',\n",
       " 'honey',\n",
       " 'butter',\n",
       " 'olive oil',\n",
       " 'salt']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['parsed'].iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dtype('O')"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['parsed'].dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of corpus: 94564\n"
     ]
    }
   ],
   "source": [
    "# get corpus with the documents sorted in alphabetical order\n",
    "def get_and_sort_corpus(data):\n",
    "    corpus_sorted = []\n",
    "    for doc in data.parsed.values:\n",
    "        doc.sort()\n",
    "        corpus_sorted.append(doc)\n",
    "    return corpus_sorted\n",
    "\n",
    "corpus = get_and_sort_corpus(data)\n",
    "print(f\"Length of corpus: {len(corpus)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8.926790321898396"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# calculate average length of each document \n",
    "lengths = [len(doc) for doc in corpus]\n",
    "avg_len = float(sum(lengths)) / len(lengths)\n",
    "avg_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train word2vec model \n",
    "sg = 0 # CBOW: build a language model that correctly predicts the center word given the context words in which the center word appears\n",
    "workers = 6 # number of CPUs\n",
    "window = 9 # window size: average length of each document \n",
    "min_count = 1 # unique ingredients are important to decide recipes \n",
    "\n",
    "model_cbow = Word2Vec(corpus, sg=sg, workers=workers, window=window, min_count=min_count, vector_size=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_cbow = Word2Vec.load('models/model_cbow.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word2Vec(vocab=9868, vector_size=100, alpha=0.025)\n"
     ]
    }
   ],
   "source": [
    "#Summarize the loaded model\n",
    "print(model_cbow)\n",
    "\n",
    "#Summarize vocabulary\n",
    "words = list(model_cbow.wv.index_to_key)\n",
    "words.sort()\n",
    "# print(words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.07361066 -1.8324972   1.1452914   0.34367687  0.2713495  -0.3425753\n",
      " -0.58229494  0.27240643  1.1556298   0.01240388 -0.8318319  -1.0165732\n",
      "  0.77112335  0.12960795 -0.9716633   0.17690742  0.8923252  -1.3945265\n",
      "  0.290436   -0.04339992  0.7580615   1.1052206   0.8737823   1.5322433\n",
      " -0.0235903  -0.7472721   0.17979541  0.75999284 -1.6583536  -0.23552205\n",
      "  1.4295099   1.3505839   1.1024392  -1.0483865   0.49111792  0.7330313\n",
      "  0.687744   -2.5379298   0.14389125 -0.5970398  -0.42096147 -0.07966862\n",
      "  0.7793348  -0.1823584   0.7442278   0.3127837  -0.00825099  0.5734534\n",
      "  1.024176    0.08931772  0.80366546  1.3342966   0.43076077 -1.2120396\n",
      " -0.8316845   0.9720223  -0.08080867  1.8078634  -0.14822097  0.41132224\n",
      "  0.06406178  1.6198484   0.39591515  0.24146229  0.162092    0.6233726\n",
      " -1.106728   -0.86356413  0.4644808   0.54935205  0.02982539 -0.00607769\n",
      " -0.28322443  0.8798804   1.063792   -1.1455724   0.6673078  -1.7586339\n",
      "  0.26613596  1.4409537   0.9870299   0.87014985 -0.377703   -0.06632984\n",
      " -0.9921979  -0.2591819  -0.8193552   0.09705846 -0.30524173  0.6220134\n",
      "  0.5665195  -0.17405932  0.49726227  0.6535042   0.00772653 -0.38060087\n",
      " -0.35823724 -1.096773   -0.09222635 -0.38493755]\n"
     ]
    }
   ],
   "source": [
    "#Acess vector for one word\n",
    "print(model_cbow.wv['cheese'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('cheddar cheese', 0.9109072685241699),\n",
       " ('colby cheese', 0.762747585773468),\n",
       " ('american cheese', 0.7504609227180481),\n",
       " ('colby monterey jack cheese', 0.7120832800865173),\n",
       " ('mild cheddar cheese', 0.6971008777618408),\n",
       " ('chihuahua cheese', 0.6925839781761169),\n",
       " ('low fat cheddar cheese', 0.6763399243354797),\n",
       " ('corn', 0.6734391450881958),\n",
       " ('cottage cheese', 0.6602693796157837),\n",
       " ('black olive', 0.6591867208480835)]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# most similar\n",
    "model_cbow.wv.most_similar(u'cheese')\n",
    "#model_cbow.wv.similarity('cheese', 'margarine')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_cbow.save('models/model_cbow.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from word2vec_rec import get_recs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # test\n",
    "    input = \"chicken thigh, onion, rice noodle, seaweed nori sheet, sesame, shallot, soy, spinach, star, tofu\"\n",
    "    rec = get_recs(input)\n",
    "    print(rec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded model\n",
      "                                          recipe               score  \\\n",
      "0                                  szechuan fish  0.9242935430919742   \n",
      "1     tod mun goong   thai fish and shrimp cakes  0.9213254375755039   \n",
      "2  copycat p f  chang s singapore street noodles  0.9169091062273881   \n",
      "3   thai chicken and eggplant  aubergine   curry  0.9158816618127215   \n",
      "4                                imperial fondue    0.91463966647726   \n",
      "\n",
      "                                         ingredients  \\\n",
      "0  red snapper,green onions,garlic,fish stock,soy...   \n",
      "1  flounder,shrimp,onion,cilantro,green beans,cor...   \n",
      "2  rice noodles,oil,shrimp,boneless skinless chic...   \n",
      "3  unsweetened coconut milk,green curry paste,bon...   \n",
      "4  boneless skinless chicken,white fish fillet,la...   \n",
      "\n",
      "                                               steps  \n",
      "0  ['in a wok brown the fish on all sides with th...  \n",
      "1  ['process the onion and cilantro until chopped...  \n",
      "2  ['boil rice stick noodles for 2 minutes or unt...  \n",
      "3  ['heat coconut milk in wok and whisk curry pas...  \n",
      "4  ['prepare the chicken , seafood , vegetables ,...  \n"
     ]
    }
   ],
   "source": [
    "input = \"chicken thigh, onion, rice noodle, seaweed nori sheet, sesame, shallot, soy, spinach, star, tofu\"\n",
    "rec = get_recs(input)\n",
    "print(rec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MeanEmbeddingVectorizer(object):\n",
    "\n",
    "\tdef __init__(self, word_model):\n",
    "\t\tself.word_model = word_model\n",
    "\t\tself.vector_size = word_model.wv.vector_size\n",
    "\n",
    "\tdef fit(self):  # comply with scikit-learn transformer requirement\n",
    "\t\treturn self\n",
    "\n",
    "\tdef transform(self, docs):  # comply with scikit-learn transformer requirement\n",
    "\t\tdoc_word_vector = self.word_average_list(docs)\n",
    "\t\treturn doc_word_vector\n",
    "\n",
    "\tdef word_average(self, sent):\n",
    "\t\t\"\"\"\n",
    "\t\tCompute average word vector for a single doc/sentence.\n",
    "\t\t:param sent: list of sentence tokens\n",
    "\t\t:return:\n",
    "\t\t\tmean: float of averaging word vectors\n",
    "\t\t\"\"\"\n",
    "\t\tmean = []\n",
    "\t\tfor word in sent:\n",
    "\t\t\tif word in self.word_model.wv.index_to_key:\n",
    "\t\t\t\tmean.append(self.word_model.wv.get_vector(word))\n",
    "\n",
    "\t\tif not mean:  # empty words\n",
    "\t\t\t# If a text is empty, return a vector of zeros.\n",
    "\t\t\tlogging.warning(\"cannot compute average owing to no vector for {}\".format(sent))\n",
    "\t\t\treturn np.zeros(self.vector_size)\n",
    "\t\telse:\n",
    "\t\t\tmean = np.array(mean).mean(axis=0)\n",
    "\t\t\treturn mean\n",
    "\n",
    "\n",
    "\tdef word_average_list(self, docs):\n",
    "\t\t\"\"\"\n",
    "\t\tCompute average word vector for multiple docs, where docs had been tokenized.\n",
    "\t\t:param docs: list of sentence in list of separated tokens\n",
    "\t\t:return:\n",
    "\t\t\tarray of average word vector in shape (len(docs),)\n",
    "\t\t\"\"\"\n",
    "\t\treturn np.vstack([self.word_average(sent) for sent in docs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:cannot compute average owing to no vector for []\n",
      "WARNING:root:cannot compute average owing to no vector for []\n",
      "WARNING:root:cannot compute average owing to no vector for []\n",
      "WARNING:root:cannot compute average owing to no vector for []\n",
      "WARNING:root:cannot compute average owing to no vector for []\n",
      "WARNING:root:cannot compute average owing to no vector for []\n",
      "WARNING:root:cannot compute average owing to no vector for []\n",
      "WARNING:root:cannot compute average owing to no vector for []\n",
      "WARNING:root:cannot compute average owing to no vector for []\n",
      "WARNING:root:cannot compute average owing to no vector for []\n",
      "WARNING:root:cannot compute average owing to no vector for []\n",
      "WARNING:root:cannot compute average owing to no vector for []\n",
      "WARNING:root:cannot compute average owing to no vector for []\n",
      "WARNING:root:cannot compute average owing to no vector for []\n",
      "WARNING:root:cannot compute average owing to no vector for []\n",
      "WARNING:root:cannot compute average owing to no vector for []\n",
      "WARNING:root:cannot compute average owing to no vector for []\n",
      "WARNING:root:cannot compute average owing to no vector for []\n",
      "WARNING:root:cannot compute average owing to no vector for []\n",
      "WARNING:root:cannot compute average owing to no vector for []\n",
      "WARNING:root:cannot compute average owing to no vector for []\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded model\n"
     ]
    }
   ],
   "source": [
    "# encode document by averaging word embeddings\n",
    "\n",
    "# load model \n",
    "loaded_model = Word2Vec.load('models/model_cbow.bin')\n",
    "if loaded_model:\n",
    "    print(\"Successfully loaded model\")\n",
    "\n",
    "mean_vec_tr = MeanEmbeddingVectorizer(loaded_model)\n",
    "doc_vec = mean_vec_tr.transform(corpus)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "65e86ed0733f1142eec8ce6d200d79e2af223aab50d601078877b07cd7cef66e"
  },
  "kernelspec": {
   "display_name": "Python 3.9.5 64-bit ('whatscooking': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
